{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3ec72866-89db-47a4-a241-246e7467738f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "try:\n",
    "    # 1. Obter o intervalo de datas (min e max) da tabela de pedidos\n",
    "    datas_pedidos = spark.table(\"medalhao.silver.ft_pedidos\").select(\n",
    "        F.min(\"pedido_compra_timestamp\").alias(\"data_min\"),\n",
    "        F.max(\"pedido_compra_timestamp\").alias(\"data_max\")\n",
    "    ).first()\n",
    "    \n",
    "    data_min = datas_pedidos[\"data_min\"].date()\n",
    "    data_max = datas_pedidos[\"data_max\"].date()\n",
    "\n",
    "    # 2. Gerar o DataFrame de calendário usando 'sequence' e 'explode'\n",
    "    df_calendario = spark.sql(f\"SELECT explode(sequence(to_date('{data_min}'), to_date('{data_max}'), interval 1 day)) AS sk_tempo\")\n",
    "\n",
    "    # 3. Enriquecer o calendário com as dimensões de tempo\n",
    "    df_dim_tempo = df_calendario.select(\n",
    "        F.col(\"sk_tempo\"),\n",
    "        F.year(\"sk_tempo\").alias(\"ano\"),\n",
    "        F.quarter(\"sk_tempo\").alias(\"trimestre\"),\n",
    "        F.month(\"sk_tempo\").alias(\"mes\"),\n",
    "        F.weekofyear(\"sk_tempo\").alias(\"semana_do_ano\"),\n",
    "        F.dayofmonth(\"sk_tempo\").alias(\"dia\"),\n",
    "        F.dayofweek(\"sk_tempo\").alias(\"dia_da_semana_num\"), # 1=Domingo, 7=Sábado\n",
    "        \n",
    "        # O Spark não traduz date_format para português, então pegamos em inglês\n",
    "        F.date_format(\"sk_tempo\", \"E\").alias(\"dia_da_semana_nome_en\"), # (ex: Mon)\n",
    "        F.date_format(\"sk_tempo\", \"MMMM\").alias(\"mes_nome_en\") # (ex: January)\n",
    "    )\n",
    "\n",
    "    # 4. Mapear nomes de dias e meses para Português\n",
    "    \n",
    "    mapa_dia_semana = F.create_map(\n",
    "        F.lit(\"Sun\"), F.lit(\"Domingo\"),\n",
    "        F.lit(\"Mon\"), F.lit(\"Segunda-feira\"),\n",
    "        F.lit(\"Tue\"), F.lit(\"Terça-feira\"),\n",
    "        F.lit(\"Wed\"), F.lit(\"Quarta-feira\"),\n",
    "        F.lit(\"Thu\"), F.lit(\"Quinta-feira\"),\n",
    "        F.lit(\"Fri\"), F.lit(\"Sexta-feira\"),\n",
    "        F.lit(\"Sat\"), F.lit(\"Sábado\")\n",
    "    )\n",
    "    \n",
    "    mapa_mes = F.create_map(\n",
    "        F.lit(\"January\"), F.lit(\"Janeiro\"),\n",
    "        F.lit(\"February\"), F.lit(\"Fevereiro\"),\n",
    "        F.lit(\"March\"), F.lit(\"Março\"),\n",
    "        F.lit(\"April\"), F.lit(\"Abril\"),\n",
    "        F.lit(\"May\"), F.lit(\"Maio\"),\n",
    "        F.lit(\"June\"), F.lit(\"Junho\"),\n",
    "        F.lit(\"July\"), F.lit(\"Julho\"),\n",
    "        F.lit(\"August\"), F.lit(\"Agosto\"),\n",
    "        F.lit(\"September\"), F.lit(\"Setembro\"),\n",
    "        F.lit(\"October\"), F.lit(\"Outubro\"),\n",
    "        F.lit(\"November\"), F.lit(\"Novembro\"),\n",
    "        F.lit(\"December\"), F.lit(\"Dezembro\")\n",
    "    )\n",
    "    \n",
    "    df_dim_tempo_pt = df_dim_tempo.withColumn(\n",
    "        \"dia_da_semana_nome\", mapa_dia_semana[F.col(\"dia_da_semana_nome_en\")]\n",
    "    ).withColumn(\n",
    "        \"mes_nome\", mapa_mes[F.col(\"mes_nome_en\")]\n",
    "    ).withColumn(\n",
    "        \"eh_fim_de_semana\",\n",
    "        F.when(F.col(\"dia_da_semana_num\").isin(1, 7), \"Sim\").otherwise(\"Não\")\n",
    "    )\n",
    "    \n",
    "    # 5. Selecionar colunas finais (conforme tabela do PDF)\n",
    "    df_final = df_dim_tempo_pt.select(\n",
    "        \"sk_tempo\",\n",
    "        \"ano\",\n",
    "        \"trimestre\",\n",
    "        \"mes\",\n",
    "        \"semana_do_ano\",\n",
    "        \"dia\",\n",
    "        \"dia_da_semana_num\",\n",
    "        \"dia_da_semana_nome\",\n",
    "        \"mes_nome\",\n",
    "        \"eh_fim_de_semana\"\n",
    "    )\n",
    "\n",
    "    df_final.write.mode(\"overwrite\").saveAsTable(\"medalhao.gold.dm_tempo\")\n",
    "    \n",
    "    print(\"dimensão medalhao.gold.dm_tempo criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"erro em 3.1: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "62e65d66-7535-4954-b1f2-73e14972a251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "try:\n",
    "    # 1. Carregar tabelas Silver\n",
    "    df_pedidos = spark.table(\"medalhao.silver.ft_pedidos\")\n",
    "    df_itens = spark.table(\"medalhao.silver.ft_itens_pedidos\")\n",
    "    df_cotacao = spark.table(\"medalhao.silver.dm_cotacao_dolar\")\n",
    "    \n",
    "    # 2. Carregar e Limpar a tabela de avaliações\n",
    "    df_avaliacoes_raw = spark.table(\"medalhao.silver.ft_avaliacoes_pedidos\")\n",
    "\n",
    "    # Usar try_cast (via F.expr) para converter a coluna 'avaliacao' para número. Textos corrompidos virarão NULL.\n",
    "    df_avaliacoes_clean = df_avaliacoes_raw.withColumn(\n",
    "        \"avaliacao_num\",\n",
    "        F.expr(\"try_cast(avaliacao AS double)\")\n",
    "    )\n",
    "    \n",
    "    # 3. Pré-processar: Calcular a média da avaliação por pedido\n",
    "    # Agora usamos a nova coluna limpa 'avaliacao_num'\n",
    "    df_avg_avaliacoes = df_avaliacoes_clean.groupBy(\"id_pedido\") \\\n",
    "                                     .agg(F.avg(\"avaliacao_num\").alias(\"avaliacao_pedido\"))\n",
    "\n",
    "    # 4. Preparar a tabela de pedidos para o join com cotação\n",
    "    df_pedidos_com_data = df_pedidos.withColumn(\"fk_tempo\", F.to_date(F.col(\"pedido_compra_timestamp\")))\n",
    "\n",
    "    # 5. Juntar (join) todas as fontes de dados\n",
    "    df_joined = df_itens.join(\n",
    "        df_pedidos_com_data,\n",
    "        \"id_pedido\",\n",
    "        \"left\"\n",
    "    ).join(\n",
    "        df_cotacao,\n",
    "        df_pedidos_com_data[\"fk_tempo\"] == df_cotacao[\"data\"],\n",
    "        \"left\"\n",
    "    ).join(\n",
    "        df_avg_avaliacoes,\n",
    "        \"id_pedido\",\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    # 6. Calcular colunas de valor\n",
    "    df_final = df_joined.withColumn(\n",
    "        \"valor_total_item_brl\", (F.col(\"preco_BRL\") + F.col(\"preco_frete\"))\n",
    "    ).withColumn(\n",
    "        \"valor_produto_usd\", (F.col(\"preco_BRL\") / F.col(\"cotacao_dolar\"))\n",
    "    ).withColumn(\n",
    "        \"valor_frete_usd\", (F.col(\"preco_frete\") / F.col(\"cotacao_dolar\"))\n",
    "    ).withColumn(\n",
    "        \"valor_total_item_usd\", (F.col(\"valor_total_item_brl\") / F.col(\"cotacao_dolar\"))\n",
    "    )\n",
    "\n",
    "    # 7. Selecionar e renomear colunas finais (conforme tabela do PDF)\n",
    "    df_gold = df_final.select(\n",
    "        F.col(\"id_pedido\"),\n",
    "        F.col(\"id_item\"),\n",
    "        F.col(\"id_consumidor\").alias(\"fk_cliente\"),\n",
    "        F.col(\"id_produto\").alias(\"fk_produto\"),\n",
    "        F.col(\"id_vendedor\").alias(\"fk_vendedor\"),\n",
    "        F.col(\"fk_tempo\"),\n",
    "        F.col(\"status\").alias(\"status_pedido\"),\n",
    "        F.col(\"tempo_entrega_dias\"),\n",
    "        F.col(\"entrega_no_prazo\"),\n",
    "        \n",
    "        # Valores BRL\n",
    "        F.col(\"preco_BRL\").cast(DecimalType(12, 2)).alias(\"valor_produto_brl\"),\n",
    "        F.col(\"preco_frete\").cast(DecimalType(12, 2)).alias(\"valor_frete_brl\"),\n",
    "        F.col(\"valor_total_item_brl\").cast(DecimalType(12, 2)).alias(\"valor_total_item_brl\"),\n",
    "        \n",
    "        # Valores USD\n",
    "        F.col(\"valor_produto_usd\").cast(DecimalType(12, 2)).alias(\"valor_produto_usd\"),\n",
    "        F.col(\"valor_frete_usd\").cast(DecimalType(12, 2)).alias(\"valor_frete_usd\"),\n",
    "        F.col(\"valor_total_item_usd\").cast(DecimalType(12, 2)).alias(\"valor_total_item_usd\"),\n",
    "        \n",
    "        F.col(\"cotacao_dolar\").cast(DecimalType(8, 4)).alias(\"cotacao_dolar\"),\n",
    "        F.col(\"avaliacao_pedido\").cast(DecimalType(3, 2)).alias(\"avaliacao_pedido\")\n",
    "    )\n",
    "\n",
    "    # 8. Salvar na camada Gold\n",
    "    df_gold.write.mode(\"overwrite\").saveAsTable(\"medalhao.gold.ft_vendas_geral\")\n",
    "    \n",
    "    print(\"tabela medalhao.gold.ft_vendas_geral criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"erro em 3.2: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f0c71da0-32fa-41c9-b4d8-3506ebee97ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Query SQL para criar a view (com a coluna 'dia_da_semana_nome' adicionada)\n",
    "query_view_3_3 = \"\"\"\n",
    "    CREATE OR REPLACE VIEW medalhao.gold.view_vendas_por_periodo AS\n",
    "    SELECT \n",
    "        -- Dimensões de Tempo\n",
    "        t.ano,\n",
    "        t.trimestre,\n",
    "        t.mes,\n",
    "        t.mes_nome,\n",
    "        t.dia,\n",
    "        t.dia_da_semana_num,\n",
    "        t.dia_da_semana_nome, -- <<< CORREÇÃO: Coluna adicionada\n",
    "        \n",
    "        -- Indicadores (KPIs)\n",
    "        COUNT(DISTINCT v.id_pedido) AS total_pedidos,\n",
    "        COUNT(v.id_item) AS total_itens,\n",
    "        SUM(v.valor_total_item_brl) AS receita_total_brl,\n",
    "        SUM(v.valor_total_item_usd) AS receita_total_usd,\n",
    "        AVG(v.valor_total_item_brl) AS ticket_medio_brl,\n",
    "        AVG(v.avaliacao_pedido) AS avaliacao_media\n",
    "        \n",
    "    FROM medalhao.gold.ft_vendas_geral AS v\n",
    "    \n",
    "    -- Juntar com a dimensão de tempo\n",
    "    LEFT JOIN medalhao.gold.dm_tempo AS t\n",
    "        ON v.fk_tempo = t.sk_tempo\n",
    "        \n",
    "    GROUP BY \n",
    "        t.ano,\n",
    "        t.trimestre,\n",
    "        t.mes,\n",
    "        t.mes_nome,\n",
    "        t.dia,\n",
    "        t.dia_da_semana_num,\n",
    "        t.dia_da_semana_nome -- <<< CORREÇÃO: Coluna adicionada\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(query_view_3_3)\n",
    "    print(\"view medalhao.gold.view_vendas_por_periodo criada.\")\n",
    "except Exception as e:\n",
    "    print(f\"erro em 3.3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9a016d56-7b0e-4f87-b367-cff69df517e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Pergunta 1: Qual é o dia da semana com maior receita total em reais?\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        dia_da_semana_nome,\n",
    "        SUM(receita_total_brl) AS total_faturado\n",
    "    FROM medalhao.gold.view_vendas_por_periodo\n",
    "    GROUP BY dia_da_semana_nome\n",
    "    ORDER BY total_faturado DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "print(\"\\nPergunta 2: No último ano, qual mês teve o maior ticket médio?\")\n",
    "# Primeiro, encontramos o último ano disponível\n",
    "ultimo_ano = spark.table(\"medalhao.gold.dm_tempo\").selectExpr(\"MAX(ano)\").first()[0]\n",
    "print(f\"(Analisando o último ano: {ultimo_ano})\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        mes_nome,\n",
    "        AVG(ticket_medio_brl) AS media_ticket_medio\n",
    "    FROM medalhao.gold.view_vendas_por_periodo\n",
    "    WHERE ano = {ultimo_ano}\n",
    "    GROUP BY mes, mes_nome\n",
    "    ORDER BY media_ticket_medio DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "60c545f8-df55-4b48-bfc2-7b37c4070bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Query SQL para criar a view analítica\n",
    "# Esta view junta a fato de vendas com os dados dos produtos\n",
    "query_view_3_4 = \"\"\"\n",
    "    CREATE OR REPLACE VIEW medalhao.gold.view_top_produto AS\n",
    "    SELECT \n",
    "        v.fk_produto AS id_produto,\n",
    "        p.categoria_produto,\n",
    "        \n",
    "        -- KPIs de Vendas\n",
    "        COUNT(v.id_item) AS quantidade_vendida,\n",
    "        COUNT(DISTINCT v.id_pedido) AS total_pedidos,\n",
    "        \n",
    "        -- KPIs de Receita\n",
    "        SUM(v.valor_total_item_brl) AS receita_brl,\n",
    "        SUM(v.valor_total_item_usd) AS receita_usd,\n",
    "        AVG(v.valor_total_item_brl) AS preco_medio_brl,\n",
    "        \n",
    "        -- KPIs de Avaliação e Logística\n",
    "        AVG(v.avaliacao_pedido) AS avaliacao_media,\n",
    "        AVG(p.peso_produto_gramas) AS peso_medio_gramas\n",
    "        \n",
    "    FROM medalhao.gold.ft_vendas_geral AS v\n",
    "    \n",
    "    -- Juntar com a tabela de produtos para buscar a categoria e peso\n",
    "    LEFT JOIN medalhao.silver.ft_produtos AS p\n",
    "        ON v.fk_produto = p.id_produto\n",
    "        \n",
    "    GROUP BY\n",
    "        v.fk_produto,\n",
    "        p.categoria_produto\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(query_view_3_4)\n",
    "    print(\"view medalhao.gold.view_top_produto criada.\")\n",
    "except Exception as e:\n",
    "    print(f\"erro em 3.4: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1e389e-fd28-4ea7-8ad1-9b74770b777e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Query SQL para criar a view (usando CTE)\n",
    "query_view_3_5 = \"\"\"\n",
    "    CREATE OR REPLACE VIEW medalhao.gold.view_vendas_produtos_esteticos AS\n",
    "    \n",
    "    -- Início da CTE: Selecionar e filtrar os dados base\n",
    "    WITH VendasFashion AS (\n",
    "        SELECT \n",
    "            t.ano,\n",
    "            t.mes,\n",
    "            p.categoria_produto,\n",
    "            v.id_pedido,\n",
    "            v.id_item,\n",
    "            v.valor_total_item_brl,\n",
    "            v.valor_total_item_usd,\n",
    "            v.avaliacao_pedido\n",
    "        FROM medalhao.gold.ft_vendas_geral AS v\n",
    "        \n",
    "        -- Join para buscar categoria\n",
    "        LEFT JOIN medalhao.silver.ft_produtos AS p\n",
    "            ON v.fk_produto = p.id_produto\n",
    "            \n",
    "        -- Join para buscar ano/mês\n",
    "        LEFT JOIN medalhao.gold.dm_tempo AS t\n",
    "            ON v.fk_tempo = t.sk_tempo\n",
    "            \n",
    "        -- Filtro da categoria \"Fashion\"\n",
    "        WHERE p.categoria_produto LIKE 'fashion%'\n",
    "    )\n",
    "    \n",
    "    -- Query principal: Agregar os dados da CTE\n",
    "    SELECT\n",
    "        ano,\n",
    "        mes,\n",
    "        categoria_produto,\n",
    "        \n",
    "        -- KPIs\n",
    "        COUNT(DISTINCT id_pedido) AS total_pedidos,\n",
    "        COUNT(id_item) AS total_itens_vendidos,\n",
    "        SUM(valor_total_item_brl) AS receita_total_brl,\n",
    "        SUM(valor_total_item_usd) AS receita_total_usd,\n",
    "        AVG(valor_total_item_brl) AS ticket_medio_brl,\n",
    "        AVG(valor_total_item_usd) AS ticket_medio_usd,\n",
    "        AVG(avaliacao_pedido) AS avaliacao_media\n",
    "        \n",
    "    FROM VendasFashion\n",
    "    GROUP BY\n",
    "        ano,\n",
    "        mes,\n",
    "        categoria_produto\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(query_view_3_5)\n",
    "    print(\"view medalhao.gold.view_vendas_produtos_esteticos criada.\")\n",
    "except Exception as e:\n",
    "    print(f\"erro em 3.5: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "projeto3_gold_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
