{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9bb4c71b-f926-40a7-bb21-efe12775a8a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Iniciando a transformação para silver.ft_consumidores...\")\n",
    "\n",
    "try:\n",
    "    # 1. Ler a tabela da camada Bronze\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_consumidores\")\n",
    "\n",
    "    # 2. Remover duplicatas com base no 'customer_id' \n",
    "    # Usamos dropDuplicates para manter apenas um registro por id_consumidor\n",
    "    df_deduplicated = df_bronze.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "    # 3. Aplicar transformações\n",
    "    df_silver = df_deduplicated.select(\n",
    "        # Renomear colunas \n",
    "        F.col(\"customer_id\").alias(\"id_consumidor\"),\n",
    "        F.col(\"customer_zip_code_prefix\").alias(\"prefixo_cep\"),\n",
    "        \n",
    "        # Converter para maiúsculas (UPPER CASE) \n",
    "        F.upper(F.col(\"customer_city\")).alias(\"cidade\"),\n",
    "        F.upper(F.col(\"customer_state\")).alias(\"estado\")\n",
    "    )\n",
    "\n",
    "    # 4. Salvar a tabela limpa na camada Silver\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_consumidores\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.ft_consumidores criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao processar silver.ft_consumidores: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "da3bd162-1205-4d1b-aa04-6dc980f67408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Iniciando a transformação para silver.ft_pedidos...\")\n",
    "\n",
    "try:\n",
    "    # 1. Ler a tabela da camada Bronze\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_pedidos\")\n",
    "\n",
    "    # 2. Criar o dicionário de mapeamento para tradução de status\n",
    "    status_map = {\n",
    "        \"delivered\": \"entregue\",\n",
    "        \"invoiced\": \"faturado\",\n",
    "        \"shipped\": \"enviado\",\n",
    "        \"processing\": \"em processamento\",\n",
    "        \"unavailable\": \"indisponível\",\n",
    "        \"canceled\": \"cancelado\",\n",
    "        \"created\": \"criado\",\n",
    "        \"approved\": \"aprovado\"\n",
    "    }\n",
    "    \n",
    "    # --- CORREÇÃO AQUI ---\n",
    "    # A forma correta de criar um mapa de tradução no Spark\n",
    "    # é \"achatar\" (flatten) o dicionário em uma lista de literais\n",
    "    \n",
    "    # 1. Achatar o dicionário para a lista: [key1, val1, key2, val2, ...]\n",
    "    map_args = []\n",
    "    for key, value in status_map.items():\n",
    "        map_args.append(F.lit(key))\n",
    "        map_args.append(F.lit(value))\n",
    "    \n",
    "    # 2. Criar a coluna do tipo \"mapa\"\n",
    "    map_expr = F.create_map(*map_args)\n",
    "    # --- FIM DA CORREÇÃO ---\n",
    "\n",
    "    # 3. Enriquecimento: Criar colunas de tempo (código original, está correto)\n",
    "    df_enriched = df_bronze.withColumn(\n",
    "        \"tempo_entrega_dias\", \n",
    "        F.datediff(F.col(\"order_delivered_customer_date\"), F.col(\"order_purchase_timestamp\"))\n",
    "    ).withColumn(\n",
    "        \"tempo_entrega_estimado_dias\",\n",
    "        F.datediff(F.col(\"order_estimated_delivery_date\"), F.col(\"order_purchase_timestamp\"))\n",
    "    )\n",
    "    \n",
    "    # Calcular a diferença de entrega\n",
    "    df_enriched = df_enriched.withColumn(\n",
    "        \"diferenca_entrega_dias\", \n",
    "        F.col(\"tempo_entrega_dias\") - F.col(\"tempo_entrega_estimado_dias\")\n",
    "    )\n",
    "\n",
    "    # 4. Enriquecimento: Lógica da coluna 'entrega_no_prazo' (código original, está correto)\n",
    "    entrega_prazo_logic = (\n",
    "        F.when(F.col(\"order_delivered_customer_date\").isNull(), \"Não Entregue\")\n",
    "         .when(F.col(\"diferenca_entrega_dias\") <= 0, \"Sim\")\n",
    "         .otherwise(\"Não\")\n",
    "    )\n",
    "\n",
    "    # 5. Aplicar transformações, renomear e selecionar colunas finais\n",
    "    df_silver = df_enriched.select(\n",
    "        F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "        F.col(\"customer_id\").alias(\"id_consumidor\"),\n",
    "        \n",
    "        # --- CORREÇÃO AQUI ---\n",
    "        # Usar a coluna do mapa para \"procurar\" o valor traduzido\n",
    "        # F.coalesce() garante que se a tradução não for encontrada (nulo), \n",
    "        # o valor original (em inglês) seja mantido.\n",
    "        F.coalesce(map_expr[F.col(\"order_status\")], F.col(\"order_status\")).alias(\"status\"),\n",
    "        # --- FIM DA CORREÇÃO ---\n",
    "        \n",
    "        F.col(\"order_purchase_timestamp\").alias(\"pedido_compra_timestamp\"),\n",
    "        F.col(\"order_approved_at\").alias(\"pedido_aprovado_timestamp\"),\n",
    "        F.col(\"order_delivered_carrier_date\").alias(\"pedido_carregado_timestamp\"),\n",
    "        F.col(\"order_delivered_customer_date\").alias(\"pedido_entregue_timestamp\"),\n",
    "        F.col(\"order_estimated_delivery_date\").alias(\"pedido_estimativa_entrega_timestamp\"),\n",
    "        \n",
    "        # Novas colunas\n",
    "        F.col(\"tempo_entrega_dias\"),\n",
    "        F.col(\"tempo_entrega_estimado_dias\"),\n",
    "        F.col(\"diferenca_entrega_dias\"),\n",
    "        entrega_prazo_logic.alias(\"entrega_no_prazo\")\n",
    "    )\n",
    "\n",
    "    # 6. Salvar a tabela limpa na camada Silver\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_pedidos\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.ft_pedidos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao processar silver.ft_pedidos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43e00f19-87d4-4c81-b52a-bd282b3c4e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "print(\"Iniciando a transformação para silver.ft_itens_pedidos...\")\n",
    "\n",
    "try:\n",
    "    # 1. Ler a tabela da camada Bronze\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_itens_pedidos\")\n",
    "\n",
    "    # 2. Aplicar transformações: Renomear e ajustar tipos\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "        F.col(\"order_item_id\").alias(\"id_item\"),\n",
    "        F.col(\"product_id\").alias(\"id_produto\"),\n",
    "        F.col(\"seller_id\").alias(\"id_vendedor\"),\n",
    "        \n",
    "        # Ajustar tipo para monetário\n",
    "        F.col(\"price\").cast(DecimalType(12, 2)).alias(\"preco_BRL\"),\n",
    "        F.col(\"freight_value\").cast(DecimalType(12, 2)).alias(\"preco_frete\")\n",
    "    )\n",
    "\n",
    "    # 3. Salvar a tabela limpa na camada Silver\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_itens_pedidos\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.ft_itens_pedidos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao processar silver.ft_itens_pedidos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2d360820-cfa8-46ab-a040-b6704c137984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "print(\"Iniciando a transformação para silver.ft_pagamentos...\")\n",
    "\n",
    "try:\n",
    "    # 1. Ler a tabela da camada Bronze\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_pagamentos_pedidos\")\n",
    "\n",
    "    # 2. Criar o dicionário de mapeamento para tradução da forma de pagamento\n",
    "    pagamento_map = {\n",
    "        \"credit_card\": \"Cartão de Crédito\",\n",
    "        \"boleto\": \"Boleto\",\n",
    "        \"voucher\": \"Voucher\",\n",
    "        \"debit_card\": \"Cartão de Débito\"\n",
    "    }\n",
    "    \n",
    "    # Achatar o dicionário para a lista de literais do Spark\n",
    "    map_args = []\n",
    "    for key, value in pagamento_map.items():\n",
    "        map_args.append(F.lit(key))\n",
    "        map_args.append(F.lit(value))\n",
    "    \n",
    "    # Criar a coluna do tipo \"mapa\"\n",
    "    map_expr = F.create_map(*map_args)\n",
    "\n",
    "    # 3. Aplicar transformações, renomear e selecionar colunas\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "        F.col(\"payment_sequential\").alias(\"codigo_pagamento\"),\n",
    "        \n",
    "        # Usar o mapa para traduzir.\n",
    "        # Se não encontrar no mapa, F.coalesce() usa o 'otherwise' (\"Outro\")\n",
    "        F.coalesce(map_expr[F.col(\"payment_type\")], F.lit(\"Outro\")).alias(\"forma_pagamento\"),\n",
    "        \n",
    "        F.col(\"payment_installments\").alias(\"parcelas\"),\n",
    "        F.col(\"payment_value\").cast(DecimalType(12, 2)).alias(\"valor_pagamento\")\n",
    "    )\n",
    "\n",
    "    # 4. Salvar a tabela limpa na camada Silver\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_pagamentos\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.ft_pagamentos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao processar silver.ft_pagamentos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "59ec4ce9-b5f0-45fb-91a2-c6e99018446d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Iniciando a transformação para silver.ft_avaliacoes_pedidos...\")\n",
    "\n",
    "try:\n",
    "    # 1. Ler a tabela da camada Bronze\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_avaliacoes_pedidos\")\n",
    "    \n",
    "    # 2. Contar as linhas antes da limpeza\n",
    "    linhas_antes = df_bronze.count()\n",
    "    print(f\"Linhas na Bronze: {linhas_antes}\")\n",
    "\n",
    "    # 3. --- CORREÇÃO AQUI: Usar F.expr('try_cast(...)') ---\n",
    "    # Esta é a forma SQL de chamar 'try_cast', que funciona em todas as versões.\n",
    "    \n",
    "    df_com_datas_limpas = df_bronze.withColumn(\n",
    "        \"data_comentario_limpa\",\n",
    "        F.expr(\"try_cast(review_creation_date AS timestamp)\")\n",
    "    ).withColumn(\n",
    "        \"data_resposta_limpa\",\n",
    "        F.expr(\"try_cast(review_answer_timestamp AS timestamp)\")\n",
    "    )\n",
    "\n",
    "    # 4. Regras de validação (Agora usando as colunas limpas)\n",
    "    df_limpo = df_com_datas_limpas.filter(\n",
    "        (F.col(\"order_id\").isNotNull()) &\n",
    "        (F.col(\"data_comentario_limpa\").isNotNull()) &  # Filtra datas corrompidas\n",
    "        (F.col(\"data_resposta_limpa\").isNotNull()) &    # Filtra datas corrompidas\n",
    "        (F.col(\"data_comentario_limpa\") <= F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    # 5. Contar as linhas depois e documentar\n",
    "    linhas_depois = df_limpo.count()\n",
    "    linhas_removidas = linhas_antes - linhas_depois\n",
    "    print(f\"Linhas após limpeza: {linhas_depois}\")\n",
    "    print(f\"DOCUMENTAÇÃO: {linhas_removidas} linhas removidas por ID nulo, datas nulas/corrompidas ou datas futuras.\")\n",
    "\n",
    "    # 6. Aplicar transformações: Renomear colunas\n",
    "    df_silver = df_limpo.select(\n",
    "        F.col(\"review_id\").alias(\"id_avaliacao\"),\n",
    "        F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "        F.col(\"review_score\").alias(\"avaliacao\"),\n",
    "        F.col(\"review_comment_title\").alias(\"titulo_comentario\"),\n",
    "        F.col(\"review_comment_message\").alias(\"comentario\"),\n",
    "        \n",
    "        # Usar as novas colunas limpas\n",
    "        F.col(\"data_comentario_limpa\").alias(\"data_comentario\"),\n",
    "        F.col(\"data_resposta_limpa\").alias(\"data_resposta\")\n",
    "    )\n",
    "\n",
    "    # 7. Salvar a tabela limpa na camada Silver\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_avaliacoes_pedidos\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.ft_avaliacoes_pedidos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao processar silver.ft_avaliacoes_pedidos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a6ec9540-3eb6-494a-b70b-7bf373b34cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(\"Iniciando a transformação para silver.ft_produtos...\")\n",
    "\n",
    "try:\n",
    "    # 1. Ler a tabela da camada Bronze\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_produtos\")\n",
    "\n",
    "    # 2. Aplicar transformações: Renomear e ajustar tipos\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"product_id\").alias(\"id_produto\"),\n",
    "        F.col(\"product_category_name\").alias(\"categoria_produto\"),\n",
    "        \n",
    "        # Garantir que as colunas numéricas sejam Integer\n",
    "        F.col(\"product_weight_g\").cast(IntegerType()).alias(\"peso_produto_gramas\"),\n",
    "        F.col(\"product_length_cm\").cast(IntegerType()).alias(\"comprimento_centimetros\"),\n",
    "        F.col(\"product_height_cm\").cast(IntegerType()).alias(\"altura_centimetros\"),\n",
    "        F.col(\"product_width_cm\").cast(IntegerType()).alias(\"largura_centimetros\")\n",
    "    )\n",
    "\n",
    "    # 3. Salvar a tabela limpa na camada Silver\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_produtos\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.ft_produtos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao processar silver.ft_produtos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "272a508c-bbd3-467f-a7fa-f9e11ff005a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Iniciando a transformação para silver.ft_vendedores...\")\n",
    "\n",
    "try:\n",
    "    # 1. Ler a tabela da camada Bronze\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_vendedores\")\n",
    "\n",
    "    # 2. Aplicar transformações: Renomear e converter para maiúsculas\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"seller_id\").alias(\"id_vendedor\"),\n",
    "        F.col(\"seller_zip_code_prefix\").alias(\"prefixo_cep\"),\n",
    "        \n",
    "        # Converter para maiúsculas (UPPER CASE)\n",
    "        F.upper(F.col(\"seller_city\")).alias(\"cidade\"),\n",
    "        F.upper(F.col(\"seller_state\")).alias(\"estado\")\n",
    "    )\n",
    "\n",
    "    # 3. Salvar a tabela limpa na camada Silver\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_vendedores\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.ft_vendedores criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao processar silver.ft_vendedores: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3f84199-f480-498a-98b3-af4a99563a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Iniciando a transformação para silver.dm_categoria_produtos_traducao...\")\n",
    "\n",
    "try:\n",
    "    # 1. Ler a tabela da camada Bronze\n",
    "    df_bronze = spark.table(\"medalhao.bronze.dm_categoria_produtos_traducao\")\n",
    "\n",
    "    # 2. Aplicar transformações: Renomear colunas\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"product_category_name\").alias(\"nome_produto_pt\"),\n",
    "        F.col(\"product_category_name_english\").alias(\"nome_produto_en\")\n",
    "    )\n",
    "\n",
    "    # 3. Salvar a tabela limpa na camada Silver\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.dm_categoria_produtos_traducao\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.dm_categoria_produtos_traducao criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao processar silver.dm_categoria_produtos_traducao: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3d63c80f-8ec7-4148-a834-acf1c0b0ac02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"Iniciando a transformação para silver.dm_cotacao_dolar...\")\n",
    "\n",
    "try:\n",
    "    # 1. Ler a tabela da camada Bronze\n",
    "    df_bronze = spark.table(\"medalhao.bronze.dm_cotacao_dolar\")\n",
    "\n",
    "    # 2. Limpar os dados da Bronze: Converter para data e pegar apenas a cotação e a data\n",
    "    df_cotacao_dias_uteis = df_bronze.select(\n",
    "        F.to_date(F.col(\"dataHoraCotacao\")).alias(\"data\"),\n",
    "        F.col(\"cotacaoCompra\")\n",
    "    ).distinct() # Garantir que temos apenas um valor por dia\n",
    "\n",
    "    # 3. Encontrar a data mínima e máxima para criar o calendário\n",
    "    min_max_datas = df_cotacao_dias_uteis.agg(\n",
    "        F.min(\"data\").alias(\"data_min\"),\n",
    "        F.max(\"data\").alias(\"data_max\")\n",
    "    ).first()\n",
    "    \n",
    "    data_min = min_max_datas[\"data_min\"]\n",
    "    data_max = min_max_datas[\"data_max\"]\n",
    "\n",
    "    # 4. Criar o DataFrame \"calendário\" com todos os dias\n",
    "    df_calendario = spark.sql(f\"SELECT explode(sequence(to_date('{data_min}'), to_date('{data_max}'), interval 1 day)) AS data\")\n",
    "\n",
    "    # 5. Fazer o left_join para criar os 'null's nos fins de semana\n",
    "    df_com_nulos = df_calendario.join(\n",
    "        df_cotacao_dias_uteis,\n",
    "        on=\"data\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 6. Usar Window Function para preencher os 'null's (Last Observation Carried Forward)\n",
    "    # \"Leve adiante\" a última cotação válida (da sexta-feira) para os dias 'null' (sábado, domingo)\n",
    "    window_spec = Window.orderBy(\"data\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    \n",
    "    df_preenchido = df_com_nulos.withColumn(\n",
    "        \"cotacao_preenchida\",\n",
    "        F.last(F.col(\"cotacaoCompra\"), ignorenulls=True).over(window_spec)\n",
    "    )\n",
    "\n",
    "    # 7. Selecionar colunas finais e salvar na Silver\n",
    "    df_silver = df_preenchido.select(\n",
    "        F.col(\"cotacao_preenchida\").alias(\"cotacao_dolar\"), # [cite: 211, 212]\n",
    "        F.col(\"data\") # [cite: 213]\n",
    "    )\n",
    "    \n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.dm_cotacao_dolar\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.dm_cotacao_dolar criada e preenchida.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao processar silver.dm_cotacao_dolar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "53479a53-1413-473c-9830-9a5fcef2f086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Iniciando validações de integridade referencial (Anti-Joins)...\")\n",
    "\n",
    "try:\n",
    "    # 1. Carregar as tabelas Silver necessárias\n",
    "    df_pedidos = spark.table(\"medalhao.silver.ft_pedidos\")\n",
    "    df_consumidores = spark.table(\"medalhao.silver.ft_consumidores\")\n",
    "    df_itens_pedidos = spark.table(\"medalhao.silver.ft_itens_pedidos\")\n",
    "\n",
    "    # --- Verificação 1: Pedidos órfãos (sem consumidor) ---\n",
    "    \n",
    "    # Usar 'left_anti' para encontrar pedidos que NÃO TÊM correspondência em consumidores\n",
    "    pedidos_orfos = df_pedidos.join(\n",
    "        df_consumidores,\n",
    "        on=\"id_consumidor\",\n",
    "        how=\"left_anti\" \n",
    "    )\n",
    "    \n",
    "    count_pedidos_orfos = pedidos_orfos.count()\n",
    "    print(f\"Verificação 1: {count_pedidos_orfos} pedidos órfãos (sem consumidor) encontrados.\")\n",
    "\n",
    "    # --- Verificação 2: Itens órfãos (sem pedido) ---\n",
    "    \n",
    "    # Usar 'left_anti' para encontrar itens que NÃO TÊM correspondência em pedidos\n",
    "    itens_orfos = df_itens_pedidos.join(\n",
    "        df_pedidos,\n",
    "        on=\"id_pedido\",\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "    \n",
    "    count_itens_orfos = itens_orfos.count()\n",
    "    print(f\"Verificação 2: {count_itens_orfos} itens de pedidos órfãos (sem pedido) encontrados.\")\n",
    "\n",
    "    # --- Remoção de Registros Órfãos (se existirem) ---\n",
    "    \n",
    "    if count_pedidos_orfos > 0:\n",
    "        print(f\"Removendo {count_pedidos_orfos} pedidos órfãos...\")\n",
    "        # 'left_semi' é o oposto do 'left_anti'. Ele MANTÉM apenas os registros que TÊM correspondência.\n",
    "        df_pedidos_limpos = df_pedidos.join(df_consumidores, on=\"id_consumidor\", how=\"left_semi\")\n",
    "        \n",
    "        # Sobrescrever a tabela silver.ft_pedidos apenas com os registros válidos\n",
    "        df_pedidos_limpos.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_pedidos\")\n",
    "        print(\"Tabela medalhao.silver.ft_pedidos atualizada.\")\n",
    "\n",
    "    if count_itens_orfos > 0:\n",
    "        print(f\"Removendo {count_itens_orfos} itens órfãos...\")\n",
    "        # Manter apenas os itens que TÊM correspondência em pedidos\n",
    "        df_itens_limpos = df_itens_pedidos.join(df_pedidos, on=\"id_pedido\", how=\"left_semi\")\n",
    "        \n",
    "        # Sobrescrever a tabela silver.ft_itens_pedidos apenas com os registros válidos\n",
    "        df_itens_limpos.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_itens_pedidos\")\n",
    "        print(\"Tabela medalhao.silver.ft_itens_pedidos atualizada.\")\n",
    "\n",
    "    if count_pedidos_orfos == 0 and count_itens_orfos == 0:\n",
    "        print(\"Nenhum registro órfão encontrado. Nenhuma remoção necessária.\")\n",
    "\n",
    "    print(\"SUCESSO: Validações de integridade concluídas.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO durante a validação: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8bfcac61-e532-47a2-824a-7c930493cda8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Iniciando a criação da tabela final silver.pedido_total...\")\n",
    "\n",
    "try:\n",
    "    # 1. Carregar as tabelas Silver necessárias\n",
    "    # (O PDF sugere bronze[cite: 224], mas usaremos as tabelas Silver limpas)\n",
    "    pedidos = spark.table(\"medalhao.silver.ft_pedidos\")\n",
    "    consumidores = spark.table(\"medalhao.silver.ft_consumidores\")\n",
    "    pagamentos = spark.table(\"medalhao.silver.ft_pagamentos\")\n",
    "    cotacao = spark.table(\"medalhao.silver.dm_cotacao_dolar\")\n",
    "\n",
    "    # 2. Agregar o valor total pago por pedido (BRL)\n",
    "    pagamentos_total_por_pedido = pagamentos.groupBy(\"id_pedido\").agg(\n",
    "        F.sum(\"valor_pagamento\").alias(\"valor_total_pago_brl\")\n",
    "    )\n",
    "\n",
    "    # 3. Preparar a tabela de pedidos para o join com cotação\n",
    "    # (Precisamos de uma coluna 'date' limpa)\n",
    "    pedidos_com_data = pedidos.withColumn(\n",
    "        \"data_pedido\", \n",
    "        F.to_date(F.col(\"pedido_compra_timestamp\"))\n",
    "    )\n",
    "\n",
    "    # 4. Juntar as tabelas\n",
    "    df_joined = pedidos_com_data.join(\n",
    "        consumidores,\n",
    "        on=\"id_consumidor\",\n",
    "        how=\"left\"\n",
    "    ).join(\n",
    "        pagamentos_total_por_pedido,\n",
    "        on=\"id_pedido\",\n",
    "        how=\"left\"\n",
    "    ).join(\n",
    "        cotacao,\n",
    "        on=pedidos_com_data.data_pedido == cotacao.data, # Join pela data do pedido\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 5. Criar colunas finais e calcular o valor em USD\n",
    "    df_silver_final = df_joined.select(\n",
    "        F.col(\"data_pedido\"),\n",
    "        F.col(\"id_pedido\"),\n",
    "        F.col(\"id_consumidor\"),\n",
    "        F.col(\"status\"),\n",
    "        F.col(\"valor_total_pago_brl\"),\n",
    "        \n",
    "        # Calcular valor em USD e arredondar para 2 casas decimais\n",
    "        (F.round(F.col(\"valor_total_pago_brl\") / F.col(\"cotacao_dolar\"), 2)).alias(\"valor_total_pago_usd\")\n",
    "    )\n",
    "\n",
    "    # 6. Salvar a tabela final na camada Silver\n",
    "    df_silver_final.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.pedido_total\")\n",
    "    \n",
    "    print(\"SUCESSO: Tabela medalhao.silver.pedido_total criada.\")\n",
    "    \n",
    "    # 7. (Opcional) Mostrar um exemplo dos dados\n",
    "    print(\"Exemplo de dados da tabela final:\")\n",
    "    df_silver_final.show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO ao criar silver.pedido_total: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Atividade2_bronze_to_silver.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
