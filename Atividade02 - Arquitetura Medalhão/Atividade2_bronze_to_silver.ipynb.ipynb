{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9bb4c71b-f926-40a7-bb21-efe12775a8a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_consumidores\")\n",
    "\n",
    "    df_deduplicated = df_bronze.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "    df_silver = df_deduplicated.select(\n",
    "        F.col(\"customer_id\").alias(\"id_consumidor\"),\n",
    "        F.col(\"customer_zip_code_prefix\").alias(\"prefixo_cep\"),\n",
    "        \n",
    "        F.upper(F.col(\"customer_city\")).alias(\"cidade\"),\n",
    "        F.upper(F.col(\"customer_state\")).alias(\"estado\")\n",
    "    )\n",
    "\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_consumidores\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.ft_consumidores criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar silver.ft_consumidores: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "da3bd162-1205-4d1b-aa04-6dc980f67408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_pedidos\")\n",
    "\n",
    "    status_map = {\n",
    "        \"delivered\": \"entregue\",\n",
    "        \"invoiced\": \"faturado\",\n",
    "        \"shipped\": \"enviado\",\n",
    "        \"processing\": \"em processamento\",\n",
    "        \"unavailable\": \"indisponível\",\n",
    "        \"canceled\": \"cancelado\",\n",
    "        \"created\": \"criado\",\n",
    "        \"approved\": \"aprovado\"\n",
    "    }\n",
    "\n",
    "    map_args = []\n",
    "    for key, value in status_map.items():\n",
    "        map_args.append(F.lit(key))\n",
    "        map_args.append(F.lit(value))\n",
    "    \n",
    "    map_expr = F.create_map(*map_args)\n",
    "\n",
    "    df_enriched = df_bronze.withColumn(\n",
    "        \"tempo_entrega_dias\", \n",
    "        F.datediff(F.col(\"order_delivered_customer_date\"), F.col(\"order_purchase_timestamp\"))\n",
    "    ).withColumn(\n",
    "        \"tempo_entrega_estimado_dias\",\n",
    "        F.datediff(F.col(\"order_estimated_delivery_date\"), F.col(\"order_purchase_timestamp\"))\n",
    "    )\n",
    "    \n",
    "    df_enriched = df_enriched.withColumn(\n",
    "        \"diferenca_entrega_dias\", \n",
    "        F.col(\"tempo_entrega_dias\") - F.col(\"tempo_entrega_estimado_dias\")\n",
    "    )\n",
    "\n",
    "    entrega_prazo_logic = (\n",
    "        F.when(F.col(\"order_delivered_customer_date\").isNull(), \"Não Entregue\")\n",
    "         .when(F.col(\"diferenca_entrega_dias\") <= 0, \"Sim\")\n",
    "         .otherwise(\"Não\")\n",
    "    )\n",
    "\n",
    "    df_silver = df_enriched.select(\n",
    "        F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "        F.col(\"customer_id\").alias(\"id_consumidor\"),\n",
    "        \n",
    "        F.coalesce(map_expr[F.col(\"order_status\")], F.col(\"order_status\")).alias(\"status\"),\n",
    "        \n",
    "        F.col(\"order_purchase_timestamp\").alias(\"pedido_compra_timestamp\"),\n",
    "        F.col(\"order_approved_at\").alias(\"pedido_aprovado_timestamp\"),\n",
    "        F.col(\"order_delivered_carrier_date\").alias(\"pedido_carregado_timestamp\"),\n",
    "        F.col(\"order_delivered_customer_date\").alias(\"pedido_entregue_timestamp\"),\n",
    "        F.col(\"order_estimated_delivery_date\").alias(\"pedido_estimativa_entrega_timestamp\"),\n",
    "        \n",
    "        F.col(\"tempo_entrega_dias\"),\n",
    "        F.col(\"tempo_entrega_estimado_dias\"),\n",
    "        F.col(\"diferenca_entrega_dias\"),\n",
    "        entrega_prazo_logic.alias(\"entrega_no_prazo\")\n",
    "    )\n",
    "\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_pedidos\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.ft_pedidos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar silver.ft_pedidos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43e00f19-87d4-4c81-b52a-bd282b3c4e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_itens_pedidos\")\n",
    "\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "        F.col(\"order_item_id\").alias(\"id_item\"),\n",
    "        F.col(\"product_id\").alias(\"id_produto\"),\n",
    "        F.col(\"seller_id\").alias(\"id_vendedor\"),\n",
    "        \n",
    "        F.col(\"price\").cast(DecimalType(12, 2)).alias(\"preco_BRL\"),\n",
    "        F.col(\"freight_value\").cast(DecimalType(12, 2)).alias(\"preco_frete\")\n",
    "    )\n",
    "\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_itens_pedidos\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.ft_itens_pedidos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar silver.ft_itens_pedidos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2d360820-cfa8-46ab-a040-b6704c137984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_pagamentos_pedidos\")\n",
    "\n",
    "    pagamento_map = {\n",
    "        \"credit_card\": \"Cartão de Crédito\",\n",
    "        \"boleto\": \"Boleto\",\n",
    "        \"voucher\": \"Voucher\",\n",
    "        \"debit_card\": \"Cartão de Débito\"\n",
    "    }\n",
    "    \n",
    "    map_args = []\n",
    "    for key, value in pagamento_map.items():\n",
    "        map_args.append(F.lit(key))\n",
    "        map_args.append(F.lit(value))\n",
    "    \n",
    "    map_expr = F.create_map(*map_args)\n",
    "\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "        F.col(\"payment_sequential\").alias(\"codigo_pagamento\"),\n",
    "        \n",
    "        F.coalesce(map_expr[F.col(\"payment_type\")], F.lit(\"Outro\")).alias(\"forma_pagamento\"),\n",
    "        \n",
    "        F.col(\"payment_installments\").alias(\"parcelas\"),\n",
    "        F.col(\"payment_value\").cast(DecimalType(12, 2)).alias(\"valor_pagamento\")\n",
    "    )\n",
    "\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_pagamentos\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.ft_pagamentos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar silver.ft_pagamentos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "59ec4ce9-b5f0-45fb-91a2-c6e99018446d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_avaliacoes_pedidos\")\n",
    "    \n",
    "    linhas_antes = df_bronze.count()\n",
    "    print(f\"Linhas na Bronze: {linhas_antes}\")\n",
    "\n",
    "    df_com_datas_limpas = df_bronze.withColumn(\n",
    "        \"data_comentario_limpa\",\n",
    "        F.expr(\"try_cast(review_creation_date AS timestamp)\")\n",
    "    ).withColumn(\n",
    "        \"data_resposta_limpa\",\n",
    "        F.expr(\"try_cast(review_answer_timestamp AS timestamp)\")\n",
    "    )\n",
    "\n",
    "    df_limpo = df_com_datas_limpas.filter(\n",
    "        (F.col(\"order_id\").isNotNull()) &\n",
    "        (F.col(\"data_comentario_limpa\").isNotNull()) &  \n",
    "        (F.col(\"data_resposta_limpa\").isNotNull()) &   \n",
    "        (F.col(\"data_comentario_limpa\") <= F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    linhas_depois = df_limpo.count()\n",
    "    linhas_removidas = linhas_antes - linhas_depois\n",
    "    print(f\"Linhas após limpeza: {linhas_depois}\")\n",
    "    print(f\"DOCUMENTAÇÃO: {linhas_removidas} linhas removidas por ID nulo, datas nulas/corrompidas ou datas futuras.\")\n",
    "\n",
    "    df_silver = df_limpo.select(\n",
    "        F.col(\"review_id\").alias(\"id_avaliacao\"),\n",
    "        F.col(\"order_id\").alias(\"id_pedido\"),\n",
    "        F.col(\"review_score\").alias(\"avaliacao\"),\n",
    "        F.col(\"review_comment_title\").alias(\"titulo_comentario\"),\n",
    "        F.col(\"review_comment_message\").alias(\"comentario\"),\n",
    "        \n",
    "        F.col(\"data_comentario_limpa\").alias(\"data_comentario\"),\n",
    "        F.col(\"data_resposta_limpa\").alias(\"data_resposta\")\n",
    "    )\n",
    "\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_avaliacoes_pedidos\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.ft_avaliacoes_pedidos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar silver.ft_avaliacoes_pedidos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a6ec9540-3eb6-494a-b70b-7bf373b34cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_produtos\")\n",
    "\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"product_id\").alias(\"id_produto\"),\n",
    "        F.col(\"product_category_name\").alias(\"categoria_produto\"),\n",
    "        \n",
    "        F.col(\"product_weight_g\").cast(IntegerType()).alias(\"peso_produto_gramas\"),\n",
    "        F.col(\"product_length_cm\").cast(IntegerType()).alias(\"comprimento_centimetros\"),\n",
    "        F.col(\"product_height_cm\").cast(IntegerType()).alias(\"altura_centimetros\"),\n",
    "        F.col(\"product_width_cm\").cast(IntegerType()).alias(\"largura_centimetros\")\n",
    "    )\n",
    "\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_produtos\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.ft_produtos criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar silver.ft_produtos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "272a508c-bbd3-467f-a7fa-f9e11ff005a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(\"medalhao.bronze.ft_vendedores\")\n",
    "\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"seller_id\").alias(\"id_vendedor\"),\n",
    "        F.col(\"seller_zip_code_prefix\").alias(\"prefixo_cep\"),\n",
    "        \n",
    "        F.upper(F.col(\"seller_city\")).alias(\"cidade\"),\n",
    "        F.upper(F.col(\"seller_state\")).alias(\"estado\")\n",
    "    )\n",
    "\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_vendedores\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.ft_vendedores criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar silver.ft_vendedores: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f84199-f480-498a-98b3-af4a99563a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(\"medalhao.bronze.dm_categoria_produtos_traducao\")\n",
    "\n",
    "    df_silver = df_bronze.select(\n",
    "        F.col(\"product_category_name\").alias(\"nome_produto_pt\"),\n",
    "        F.col(\"product_category_name_english\").alias(\"nome_produto_en\")\n",
    "    )\n",
    "\n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.dm_categoria_produtos_traducao\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.dm_categoria_produtos_traducao criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar silver.dm_categoria_produtos_traducao: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3d63c80f-8ec7-4148-a834-acf1c0b0ac02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "try:\n",
    "    df_bronze = spark.table(\"medalhao.bronze.dm_cotacao_dolar\")\n",
    "\n",
    "    df_cotacao_dias_uteis = df_bronze.select(\n",
    "        F.to_date(F.col(\"dataHoraCotacao\")).alias(\"data\"),\n",
    "        F.col(\"cotacaoCompra\")\n",
    "    ).distinct() \n",
    "\n",
    "    min_max_datas = df_cotacao_dias_uteis.agg(\n",
    "        F.min(\"data\").alias(\"data_min\"),\n",
    "        F.max(\"data\").alias(\"data_max\")\n",
    "    ).first()\n",
    "    \n",
    "    data_min = min_max_datas[\"data_min\"]\n",
    "    data_max = min_max_datas[\"data_max\"]\n",
    "\n",
    "    df_calendario = spark.sql(f\"SELECT explode(sequence(to_date('{data_min}'), to_date('{data_max}'), interval 1 day)) AS data\")\n",
    "\n",
    "    df_com_nulos = df_calendario.join(\n",
    "        df_cotacao_dias_uteis,\n",
    "        on=\"data\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    window_spec = Window.orderBy(\"data\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    \n",
    "    df_preenchido = df_com_nulos.withColumn(\n",
    "        \"cotacao_preenchida\",\n",
    "        F.last(F.col(\"cotacaoCompra\"), ignorenulls=True).over(window_spec)\n",
    "    )\n",
    "\n",
    "    df_silver = df_preenchido.select(\n",
    "        F.col(\"cotacao_preenchida\").alias(\"cotacao_dolar\"), \n",
    "        F.col(\"data\") \n",
    "    )\n",
    "    \n",
    "    df_silver.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.dm_cotacao_dolar\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.dm_cotacao_dolar criada e preenchida.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar silver.dm_cotacao_dolar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "53479a53-1413-473c-9830-9a5fcef2f086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "try:\n",
    "    df_pedidos = spark.table(\"medalhao.silver.ft_pedidos\")\n",
    "    df_consumidores = spark.table(\"medalhao.silver.ft_consumidores\")\n",
    "    df_itens_pedidos = spark.table(\"medalhao.silver.ft_itens_pedidos\")\n",
    "\n",
    "    pedidos_orfos = df_pedidos.join(\n",
    "        df_consumidores,\n",
    "        on=\"id_consumidor\",\n",
    "        how=\"left_anti\" \n",
    "    )\n",
    "    \n",
    "    count_pedidos_orfos = pedidos_orfos.count()\n",
    "    print(f\"Verificação 1: {count_pedidos_orfos} pedidos órfãos (sem consumidor) encontrados.\")\n",
    "\n",
    "    itens_orfos = df_itens_pedidos.join(\n",
    "        df_pedidos,\n",
    "        on=\"id_pedido\",\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "    \n",
    "    count_itens_orfos = itens_orfos.count()\n",
    "    print(f\"Verificação 2: {count_itens_orfos} itens de pedidos órfãos (sem pedido) encontrados.\")\n",
    "\n",
    "    \n",
    "    if count_pedidos_orfos > 0:\n",
    "        print(f\"Removendo {count_pedidos_orfos} pedidos órfãos...\")\n",
    "        df_pedidos_limpos = df_pedidos.join(df_consumidores, on=\"id_consumidor\", how=\"left_semi\")\n",
    "        \n",
    "        df_pedidos_limpos.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_pedidos\")\n",
    "        print(\"Tabela medalhao.silver.ft_pedidos atualizada.\")\n",
    "\n",
    "    if count_itens_orfos > 0:\n",
    "        print(f\"Removendo {count_itens_orfos} itens órfãos...\")\n",
    "        df_itens_limpos = df_itens_pedidos.join(df_pedidos, on=\"id_pedido\", how=\"left_semi\")\n",
    "        \n",
    "        df_itens_limpos.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.ft_itens_pedidos\")\n",
    "        print(\"Tabela medalhao.silver.ft_itens_pedidos atualizada.\")\n",
    "\n",
    "    if count_pedidos_orfos == 0 and count_itens_orfos == 0:\n",
    "        print(\"Nenhum registro órfão encontrado.\")\n",
    "\n",
    "    print(\"Validações de integridade concluídas.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro durante a validação: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bfcac61-e532-47a2-824a-7c930493cda8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "try:\n",
    "    pedidos = spark.table(\"medalhao.silver.ft_pedidos\")\n",
    "    consumidores = spark.table(\"medalhao.silver.ft_consumidores\")\n",
    "    pagamentos = spark.table(\"medalhao.silver.ft_pagamentos\")\n",
    "    cotacao = spark.table(\"medalhao.silver.dm_cotacao_dolar\")\n",
    "\n",
    "    pagamentos_total_por_pedido = pagamentos.groupBy(\"id_pedido\").agg(\n",
    "        F.sum(\"valor_pagamento\").alias(\"valor_total_pago_brl\")\n",
    "    )\n",
    "\n",
    "    pedidos_com_data = pedidos.withColumn(\n",
    "        \"data_pedido\", \n",
    "        F.to_date(F.col(\"pedido_compra_timestamp\"))\n",
    "    )\n",
    "\n",
    "    df_joined = pedidos_com_data.join(\n",
    "        consumidores,\n",
    "        on=\"id_consumidor\",\n",
    "        how=\"left\"\n",
    "    ).join(\n",
    "        pagamentos_total_por_pedido,\n",
    "        on=\"id_pedido\",\n",
    "        how=\"left\"\n",
    "    ).join(\n",
    "        cotacao,\n",
    "        on=pedidos_com_data.data_pedido == cotacao.data,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_silver_final = df_joined.select(\n",
    "        F.col(\"data_pedido\"),\n",
    "        F.col(\"id_pedido\"),\n",
    "        F.col(\"id_consumidor\"),\n",
    "        F.col(\"status\"),\n",
    "        F.col(\"valor_total_pago_brl\"),\n",
    "        \n",
    "        (F.round(F.col(\"valor_total_pago_brl\") / F.col(\"cotacao_dolar\"), 2)).alias(\"valor_total_pago_usd\")\n",
    "    )\n",
    "\n",
    "    df_silver_final.write.mode(\"overwrite\").saveAsTable(\"medalhao.silver.pedido_total\")\n",
    "    \n",
    "    print(\"Tabela medalhao.silver.pedido_total criada.\")\n",
    "    \n",
    "    print(\"Exemplo de dados da tabela final:\")\n",
    "    df_silver_final.show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao criar silver.pedido_total: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Atividade2_bronze_to_silver.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
